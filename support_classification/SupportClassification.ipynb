{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Classification\n",
    "===\n",
    "\n",
    "Project site: https://blablablab.si.umich.edu/projects/support/\n",
    "\n",
    "Paper: https://www.aclweb.org/anthology/D18-1004\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as md\n",
    "import matplotlib\n",
    "import pylab as pl\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import sklearn\n",
    "import sklearn.model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "git_root_dir = !git rev-parse --show-toplevel\n",
    "git_root_dir = Path(git_root_dir[0].strip())\n",
    "data_dir = git_root_dir / 'support_classification' / 'data'\n",
    "crowdflower_support_annotations_file = data_dir / 'crowdflower-support-annotations.csv'\n",
    "training_data_file = data_dir / 'training-data.with-ids.tsv'\n",
    "assert data_dir.exists() and crowdflower_support_annotations_file.exists() and training_data_file.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/levon003/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3020: DtypeWarning: Columns (4,18,19,20,21,23,24,25,26,34,35) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "35828"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(crowdflower_support_annotations_file)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_created_at</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_id</th>\n",
       "      <th>_missed</th>\n",
       "      <th>_started_at</th>\n",
       "      <th>_tainted</th>\n",
       "      <th>_channel</th>\n",
       "      <th>_trust</th>\n",
       "      <th>_worker_id</th>\n",
       "      <th>...</th>\n",
       "      <th>post</th>\n",
       "      <th>post_id</th>\n",
       "      <th>post_user</th>\n",
       "      <th>reply</th>\n",
       "      <th>reply_id</th>\n",
       "      <th>reply_user</th>\n",
       "      <th>source</th>\n",
       "      <th>support_gold</th>\n",
       "      <th>support_gold_reason</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1575933638</td>\n",
       "      <td>2/5/2018 06:11:28</td>\n",
       "      <td>True</td>\n",
       "      <td>3337204128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2/5/2018 06:07:30</td>\n",
       "      <td>False</td>\n",
       "      <td>clixsense</td>\n",
       "      <td>0.7727</td>\n",
       "      <td>43725761</td>\n",
       "      <td>...</td>\n",
       "      <td>Thanks, I wasnt trying to be rude- it just ann...</td>\n",
       "      <td>cj207fn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>It has to do with /r/atheism generally being a...</td>\n",
       "      <td>cj20cl7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>reddit-AskReddit-2014</td>\n",
       "      <td>2\\n3\\n4</td>\n",
       "      <td>The reply doesn't address the author</td>\n",
       "      <td>long-long</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1575933638</td>\n",
       "      <td>2/5/2018 06:33:46</td>\n",
       "      <td>True</td>\n",
       "      <td>3337235761</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2/5/2018 06:31:40</td>\n",
       "      <td>False</td>\n",
       "      <td>clixsense</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>40557293</td>\n",
       "      <td>...</td>\n",
       "      <td>Thanks, I wasnt trying to be rude- it just ann...</td>\n",
       "      <td>cj207fn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>It has to do with /r/atheism generally being a...</td>\n",
       "      <td>cj20cl7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>reddit-AskReddit-2014</td>\n",
       "      <td>2\\n3\\n4</td>\n",
       "      <td>The reply doesn't address the author</td>\n",
       "      <td>long-long</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1575933638</td>\n",
       "      <td>2/5/2018 06:49:11</td>\n",
       "      <td>True</td>\n",
       "      <td>3337257061</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2/5/2018 06:44:46</td>\n",
       "      <td>False</td>\n",
       "      <td>clixsense</td>\n",
       "      <td>0.7273</td>\n",
       "      <td>42837258</td>\n",
       "      <td>...</td>\n",
       "      <td>Thanks, I wasnt trying to be rude- it just ann...</td>\n",
       "      <td>cj207fn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>It has to do with /r/atheism generally being a...</td>\n",
       "      <td>cj20cl7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>reddit-AskReddit-2014</td>\n",
       "      <td>2\\n3\\n4</td>\n",
       "      <td>The reply doesn't address the author</td>\n",
       "      <td>long-long</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1575933638</td>\n",
       "      <td>2/5/2018 07:25:10</td>\n",
       "      <td>True</td>\n",
       "      <td>3337311785</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2/5/2018 07:22:42</td>\n",
       "      <td>False</td>\n",
       "      <td>neodev</td>\n",
       "      <td>0.7273</td>\n",
       "      <td>38644226</td>\n",
       "      <td>...</td>\n",
       "      <td>Thanks, I wasnt trying to be rude- it just ann...</td>\n",
       "      <td>cj207fn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>It has to do with /r/atheism generally being a...</td>\n",
       "      <td>cj20cl7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>reddit-AskReddit-2014</td>\n",
       "      <td>2\\n3\\n4</td>\n",
       "      <td>The reply doesn't address the author</td>\n",
       "      <td>long-long</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1575933638</td>\n",
       "      <td>2/5/2018 07:38:10</td>\n",
       "      <td>True</td>\n",
       "      <td>3337331241</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2/5/2018 07:36:06</td>\n",
       "      <td>False</td>\n",
       "      <td>elite</td>\n",
       "      <td>0.7727</td>\n",
       "      <td>43402768</td>\n",
       "      <td>...</td>\n",
       "      <td>Thanks, I wasnt trying to be rude- it just ann...</td>\n",
       "      <td>cj207fn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>It has to do with /r/atheism generally being a...</td>\n",
       "      <td>cj20cl7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>reddit-AskReddit-2014</td>\n",
       "      <td>2\\n3\\n4</td>\n",
       "      <td>The reply doesn't address the author</td>\n",
       "      <td>long-long</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     _unit_id        _created_at  _golden         _id _missed  \\\n",
       "0  1575933638  2/5/2018 06:11:28     True  3337204128     NaN   \n",
       "1  1575933638  2/5/2018 06:33:46     True  3337235761     NaN   \n",
       "2  1575933638  2/5/2018 06:49:11     True  3337257061     NaN   \n",
       "3  1575933638  2/5/2018 07:25:10     True  3337311785     NaN   \n",
       "4  1575933638  2/5/2018 07:38:10     True  3337331241     NaN   \n",
       "\n",
       "         _started_at  _tainted   _channel  _trust  _worker_id  ...  \\\n",
       "0  2/5/2018 06:07:30     False  clixsense  0.7727    43725761  ...   \n",
       "1  2/5/2018 06:31:40     False  clixsense  0.7500    40557293  ...   \n",
       "2  2/5/2018 06:44:46     False  clixsense  0.7273    42837258  ...   \n",
       "3  2/5/2018 07:22:42     False     neodev  0.7273    38644226  ...   \n",
       "4  2/5/2018 07:36:06     False      elite  0.7727    43402768  ...   \n",
       "\n",
       "                                                post  post_id post_user  \\\n",
       "0  Thanks, I wasnt trying to be rude- it just ann...  cj207fn       NaN   \n",
       "1  Thanks, I wasnt trying to be rude- it just ann...  cj207fn       NaN   \n",
       "2  Thanks, I wasnt trying to be rude- it just ann...  cj207fn       NaN   \n",
       "3  Thanks, I wasnt trying to be rude- it just ann...  cj207fn       NaN   \n",
       "4  Thanks, I wasnt trying to be rude- it just ann...  cj207fn       NaN   \n",
       "\n",
       "                                               reply  reply_id  reply_user  \\\n",
       "0  It has to do with /r/atheism generally being a...   cj20cl7         NaN   \n",
       "1  It has to do with /r/atheism generally being a...   cj20cl7         NaN   \n",
       "2  It has to do with /r/atheism generally being a...   cj20cl7         NaN   \n",
       "3  It has to do with /r/atheism generally being a...   cj20cl7         NaN   \n",
       "4  It has to do with /r/atheism generally being a...   cj20cl7         NaN   \n",
       "\n",
       "                  source  support_gold                   support_gold_reason  \\\n",
       "0  reddit-AskReddit-2014       2\\n3\\n4  The reply doesn't address the author   \n",
       "1  reddit-AskReddit-2014       2\\n3\\n4  The reply doesn't address the author   \n",
       "2  reddit-AskReddit-2014       2\\n3\\n4  The reply doesn't address the author   \n",
       "3  reddit-AskReddit-2014       2\\n3\\n4  The reply doesn't address the author   \n",
       "4  reddit-AskReddit-2014       2\\n3\\n4  The reply doesn't address the author   \n",
       "\n",
       "        type  \n",
       "0  long-long  \n",
       "1  long-long  \n",
       "2  long-long  \n",
       "3  long-long  \n",
       "4  long-long  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reddit', 12968),\n",
       " ('stackexchange', 10461),\n",
       " ('wiki', 7425),\n",
       " ('wikin', 4956),\n",
       " ('nan', 18)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([str(source).split(\"-\")[0] for source in df.source]).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reddit-AskReddit-2014', 646),\n",
       " ('reddit-AskReddit-2016', 499),\n",
       " ('reddit-pics-2017', 449),\n",
       " ('reddit-AskReddit-2013', 365),\n",
       " ('reddit-AskReddit-2017', 339),\n",
       " ('reddit-The_Donald-2017', 265),\n",
       " ('reddit-AskReddit-2015', 252),\n",
       " ('reddit-soccer-2016', 247),\n",
       " ('reddit-AdviceAnimals-2013', 243),\n",
       " ('reddit-worldnews-2015', 238),\n",
       " ('reddit-pics-2015', 230),\n",
       " ('reddit-politics-2015', 230),\n",
       " ('reddit-MMA-2016', 218),\n",
       " ('reddit-nfl-2015', 216),\n",
       " ('reddit-relationships-2015', 215),\n",
       " ('reddit-AdviceAnimals-2017', 215),\n",
       " ('reddit-gifs-2017', 214),\n",
       " ('reddit-xboxone-2016', 211),\n",
       " ('reddit-Showerthoughts-2015', 211),\n",
       " ('reddit-heroesofthestorm-2016', 196)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([s for s in df.source if str(s).startswith('reddit')]).most_common()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8979"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.groupby(by='reply_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data file\n",
    "\n",
    "https://github.com/davidjurgens/support/blob/master/data/training-data.with-ids.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9030"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(training_data_file, sep='\\t')\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agreement</th>\n",
       "      <th>offensiveness</th>\n",
       "      <th>politeness</th>\n",
       "      <th>support</th>\n",
       "      <th>post</th>\n",
       "      <th>reply</th>\n",
       "      <th>post_id</th>\n",
       "      <th>reply_id</th>\n",
       "      <th>source</th>\n",
       "      <th>platform</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>Well that didnt take long</td>\n",
       "      <td>What didnt?</td>\n",
       "      <td>cj4hzgi</td>\n",
       "      <td>cj4i1cc</td>\n",
       "      <td>reddit-worldnews-2014</td>\n",
       "      <td>reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>Im always amused when Im flying home and there...</td>\n",
       "      <td>and what people think is Vegas is really Paradise</td>\n",
       "      <td>csvv5j9</td>\n",
       "      <td>csvy89s</td>\n",
       "      <td>reddit-pics-2015</td>\n",
       "      <td>reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>There was no absence of regulation back then, ...</td>\n",
       "      <td>Which back when? Im no ballerina but I dont th...</td>\n",
       "      <td>cswfklp</td>\n",
       "      <td>cswktwq</td>\n",
       "      <td>reddit-todayilearned-2015</td>\n",
       "      <td>reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.333333</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>**YEA** but he had calvin johnson so that mean...</td>\n",
       "      <td>EXACTLY!    To all the Bears fans who say this...</td>\n",
       "      <td>c5c1eur</td>\n",
       "      <td>c5c1q9f</td>\n",
       "      <td>reddit-nfl-2012</td>\n",
       "      <td>reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>Maybe it shouldnt be as easy as Arya made it seem</td>\n",
       "      <td>well when you have magical face changing power...</td>\n",
       "      <td>dkbgylh</td>\n",
       "      <td>dkbhbxt</td>\n",
       "      <td>reddit-asoiaf-2017</td>\n",
       "      <td>reddit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   agreement  offensiveness  politeness   support  \\\n",
       "0   3.000000       1.000000    3.000000  3.000000   \n",
       "1   3.000000       1.333333    3.333333  3.000000   \n",
       "2   2.333333       1.000000    3.000000  3.000000   \n",
       "3   4.333333       3.000000    2.000000  3.666667   \n",
       "4   3.000000       3.000000    2.333333  3.000000   \n",
       "\n",
       "                                                post  \\\n",
       "0                          Well that didnt take long   \n",
       "1  Im always amused when Im flying home and there...   \n",
       "2  There was no absence of regulation back then, ...   \n",
       "3  **YEA** but he had calvin johnson so that mean...   \n",
       "4  Maybe it shouldnt be as easy as Arya made it seem   \n",
       "\n",
       "                                               reply  post_id reply_id  \\\n",
       "0                                        What didnt?  cj4hzgi  cj4i1cc   \n",
       "1  and what people think is Vegas is really Paradise  csvv5j9  csvy89s   \n",
       "2  Which back when? Im no ballerina but I dont th...  cswfklp  cswktwq   \n",
       "3  EXACTLY!    To all the Bears fans who say this...  c5c1eur  c5c1q9f   \n",
       "4  well when you have magical face changing power...  dkbgylh  dkbhbxt   \n",
       "\n",
       "                      source platform  \n",
       "0      reddit-worldnews-2014   reddit  \n",
       "1           reddit-pics-2015   reddit  \n",
       "2  reddit-todayilearned-2015   reddit  \n",
       "3            reddit-nfl-2012   reddit  \n",
       "4         reddit-asoiaf-2017   reddit  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reddit', 3015), ('stackexchange', 3013), ('wiki', 2996), ('?', 6)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(df.platform).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reddit', 3015),\n",
       " ('stackexchange', 3013),\n",
       " ('wikin', 1652),\n",
       " ('wiki', 1344),\n",
       " ('?', 6)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([str(source).split(\"-\")[0] for source in df.source]).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['source_site'] = [str(source).split(\"-\")[0] for source in df.source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3.0, 3787),\n",
       " (3.33333333333, 1823),\n",
       " (2.66666666667, 1448),\n",
       " (3.66666666667, 738),\n",
       " (2.33333333333, 577),\n",
       " (4.0, 303),\n",
       " (2.0, 175),\n",
       " (4.33333333333, 72),\n",
       " (1.6666666666699999, 68),\n",
       " (1.33333333333, 20),\n",
       " (4.66666666667, 11),\n",
       " (1.0, 4),\n",
       " (5.0, 2),\n",
       " (2.75, 1),\n",
       " (3.5, 1)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(df.support).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xs = []\n",
    "labels = []\n",
    "for platform in set(df.platform):\n",
    "    if platform == '?':\n",
    "        continue\n",
    "    x = df[df.platform == platform].support\n",
    "    xs.append(x)\n",
    "    labels.append(platform)\n",
    "plt.hist(xs, label=labels, align='left')\n",
    "plt.legend()\n",
    "plt.title(\"Average support scores by platform\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing with Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.15 s, sys: 935 ms, total: 9.08 s\n",
      "Wall time: 17.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9030it [00:49, 182.56it/s]\n"
     ]
    }
   ],
   "source": [
    "tokens = []\n",
    "embedding = []\n",
    "for doc in tqdm(nlp.pipe(df.reply, batch_size=1000, n_threads=3)):\n",
    "    tokens.append([token.text for token in doc])\n",
    "    embedding.append(doc.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = tokens\n",
    "df['embedding'] = embedding\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(data_dir / 'tokenized.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9030"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(data_dir / 'tokenized.pkl')\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                   [What, did, nt, ?]\n",
       "1    [and, what, people, think, is, Vegas, is, real...\n",
       "2    [Which, back, when, ?, I, m, no, ballerina, bu...\n",
       "3    [EXACTLY, !,    , To, all, the, Bears, fans, w...\n",
       "4    [well, when, you, have, magical, face, changin...\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [-0.06969675, 0.21982999, -0.07316275, -0.4447...\n",
       "1    [-0.091369994, 0.31280944, -0.14858837, -0.336...\n",
       "2    [0.09055326, 0.109529115, -0.13828273, -0.1614...\n",
       "3    [-0.064493835, 0.20588325, -0.17942156, -0.104...\n",
       "4    [-0.097393975, 0.14301756, -0.20241255, -0.029...\n",
       "Name: embedding, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.embedding.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification with VW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_support_to_label(support):\n",
    "    if support <= -0.67:\n",
    "        return 1  # Unsupportive\n",
    "    elif support >= 0.67:\n",
    "        return 3  # Supportive\n",
    "    else:\n",
    "        return 2  # Neutral\n",
    "    \n",
    "def map_support_to_binary_label(support):\n",
    "    # Assign positive if between 1 and 2\n",
    "    if support > 0.67:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# write the dataframe to a file in the vowpal wabbit format\n",
    "def to_vw_format(df, filename, discretize=True, binarize=False, use_embeddings=False, add_embeddings=False):\n",
    "    with open(filename, 'w') as outfile:\n",
    "        for i in range(len(df)):\n",
    "            row = df.iloc[i]\n",
    "            if binarize:\n",
    "                label = map_support_to_binary_label(row.support - 3)\n",
    "                label = \"+1\" if label == 1 else \"-1\"\n",
    "            elif discretize:\n",
    "                # we will use a multiclass model\n",
    "                label = str(map_support_to_label(row.support - 3))\n",
    "            else: # regression task\n",
    "                label = str(row.support)\n",
    "                raise NotImplemented(\"Support regression not implemented.\")\n",
    "            if use_embeddings:\n",
    "                dim_features = []\n",
    "                for i in range(len(row.embedding)):\n",
    "                    feature = \"d\" + str(i) + \":\" + str(row.embedding[i])\n",
    "                    dim_features.append(feature)\n",
    "                features = \" \".join(dim_features)\n",
    "            else:\n",
    "                features = \" \".join([token.replace(\":\", \"COLON\") for token in row.tokens])\n",
    "                if add_embeddings:\n",
    "                    dim_features = []\n",
    "                    for i in range(len(row.embedding)):\n",
    "                        feature = \"d\" + str(i) + \":\" + str(row.embedding[i])\n",
    "                        dim_features.append(feature)\n",
    "                    features += \" |E:0.01 \" + \" \".join(dim_features)\n",
    "            line = f\"{label} {row.reply_id}|R {features}\\n\"\n",
    "            outfile.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the data\n",
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([(2, 8375), (3, 388), (1, 267)])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add discretized support label to the data\n",
    "df['support_discretized'] = [map_support_to_label(s - 3) for s in df.support]\n",
    "Counter(df.support_discretized).items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7224, 1806)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# identify test data\n",
    "test_percent = 0.2\n",
    "train_df, test_df = sklearn.model_selection.train_test_split(df, test_size=test_percent)\n",
    "len(train_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most-resampled training data was replicated 43 times.\n",
      "Among rows that were replicated at least once, rows were replicated an average of 23.45 times.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(2, 6709), (3, 6038), (1, 6038)]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "sampling_strategy = 'auto'  # the default\n",
    "neutral_class_count = Counter(train_df.support_discretized)[2]\n",
    "sampling_strategy = {2: int(neutral_class_count), \n",
    "                     1: int(neutral_class_count * 0.9), \n",
    "                     3: int(neutral_class_count * 0.9)}\n",
    "\n",
    "X_resampled, y_resampled = RandomOverSampler(sampling_strategy=sampling_strategy).fit_resample(\n",
    "    train_df.index.values.reshape(-1, 1), \n",
    "    train_df.support_discretized)\n",
    "most_resampled = Counter([val[0] for val in X_resampled]).most_common()[0][1]\n",
    "mean_resampled = np.mean([val for val in Counter([val[0] for val in X_resampled]).values() if val > 1])\n",
    "print(f\"The most-resampled training data was replicated {most_resampled} times.\")\n",
    "print(f\"Among rows that were replicated at least once, rows were replicated an average of {mean_resampled:.2f} times.\")\n",
    "train_df_resampled = train_df.loc[[val[0] for val in X_resampled]]\n",
    "Counter(train_df_resampled.support_discretized).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.86 s, sys: 0 ns, total: 3.86 s\n",
      "Wall time: 3.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_filename = data_dir / 'vw_support_3class.train'\n",
    "test_filename = data_dir / 'vw_support_3class.test'\n",
    "to_vw_format(train_df_resampled, train_filename, use_embeddings=False)\n",
    "to_vw_format(test_df, test_filename, use_embeddings=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18785 /home/levon003/repos/sna-social-support/support_classification/data/vw_support_3class.train\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l {train_filename}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 61127|R no that was just me playing with java , but that program is what triggered this question . I wrote that program in hopes to try to prevent it from happening , the program was built with success , but the prevention failed . lol\r\n",
      "3 5925688|R Thank you for the input . We can agree to disagree over whether ATC bears any responsibility at all for the tragedy . However , we do agree that the pilot error was at least the primary cause , and I am in full agreement with your edit . Thank you again .\r\n",
      "2 d5womj3|R What if its not in the code because its hidden ? As in its something that s already in the code . Like it could be a pokeball in disguise sort - of like how ditto is ?\r\n",
      "2 105487|R A couple of different points . First , the five pillars and six articles of faith we currently list here are Sunni classifications . RSs devote more space to Sunni than to Shia traditions , and so should we , but we should n't present these classifications as normative . The five pillars belong to the category of ibadat , so it would be more appropriate to title this section something like \" Acts of worship \" , and add a subsection on Shia Islam , briefly noting alternative classifications . COLON I 'm not sure how to incorporate teachings on moral behavior and social service . From a traditional perspective , they belong under mu'amalat . In fact , from that perspective , both ibadat and mu'amalat should be subsections of \" Law and jurisprudence \" , but that 's not how standard modern textbooks present this material . Perhaps we need to review a few of them for guidance . I 'm frankly concerned about relying too much on Nigosian 's book . I 'm not familiar with it , but I see that Islamic ethics#Moral commandments presents moral teachings of Islam in the highly idiosyncratic formulation of \" ten commandments \" based on this book , which is a big WP COLON UNDUE violation .\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 4 {train_filename}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 2-grams for all namespaces.\n",
      "final_regressor = /home/levon003/repos/sna-social-support/support_classification/data/vw_support_3class.model\n",
      "Num weight bits = 29\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "creating cache_file = /home/levon003/repos/sna-social-support/support_classification/data/vw_support_3class.train.cache\n",
      "Reading datafile = /home/levon003/repos/sna-social-support/support_classification/data/vw_support_3class.train\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0        2        1       90\n",
      "1.000000 1.000000            2            2.0        3        2      106\n",
      "0.500000 0.000000            4            4.0        2        2      436\n",
      "0.250000 0.000000            8            8.0        2        2      222\n",
      "0.125000 0.000000           16           16.0        2        2       96\n",
      "0.062500 0.000000           32           32.0        2        2       18\n",
      "0.062500 0.062500           64           64.0        2        2      196\n",
      "0.062500 0.062500          128          128.0        2        2       32\n",
      "0.074219 0.085938          256          256.0        2        2      212\n",
      "0.072266 0.070312          512          512.0        2        2       14\n",
      "0.071289 0.070312         1024         1024.0        2        2       32\n",
      "0.071777 0.072266         2048         2048.0        2        2       68\n",
      "0.074463 0.077148         4096         4096.0        2        2      164\n",
      "0.063599 0.052734         8192         8192.0        1        1       54\n",
      "0.033813 0.004028        16384        16384.0        3        3       46\n",
      "0.027198 0.027198        32768        32768.0        3        3       28 h\n",
      "0.019093 0.010989        65536        65536.0        3        3       56 h\n",
      "0.013462 0.007831       131072       131072.0        3        3      100 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 16907\n",
      "passes used = 8\n",
      "weighted example sum = 135256.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.007455 h\n",
      "total feature number = 10428440\n"
     ]
    }
   ],
   "source": [
    "!vw -k -c -b 29 --passes 20 --oaa 3 -d {train_filename} -f {data_dir}/vw_support_3class.model --ngram 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 2-grams for all namespaces.\n",
      "only testing\n",
      "predictions = /home/levon003/repos/sna-social-support/support_classification/data/vw_support_3class.test.pred\n",
      "Num weight bits = 29\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = /home/levon003/repos/sna-social-support/support_classification/data/vw_support_3class.test\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        2        2       14\n",
      "0.500000 1.000000            2            2.0        3        2       72\n",
      "0.250000 0.000000            4            4.0        2        2       74\n",
      "0.125000 0.000000            8            8.0        2        2       72\n",
      "0.125000 0.125000           16           16.0        2        2       12\n",
      "0.156250 0.187500           32           32.0        2        3       56\n",
      "0.140625 0.125000           64           64.0        2        2       22\n",
      "0.148438 0.156250          128          128.0        2        2       12\n",
      "0.144531 0.140625          256          256.0        2        2       28\n",
      "0.119141 0.093750          512          512.0        2        2      154\n",
      "0.135742 0.152344         1024         1024.0        1        2       88\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1806\n",
      "passes used = 1\n",
      "weighted example sum = 1806.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.131229\n",
      "total feature number = 144061\n"
     ]
    }
   ],
   "source": [
    "!vw -t -i {data_dir}/vw_support_3class.model -d {test_filename} -p {test_filename}.pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 caxzw93\r\n",
      "2 8982292\r\n",
      "2 dkhlkmx\r\n",
      "2 67292\r\n",
      "2 ciw0jw3\r\n",
      "2 14559299\r\n",
      "2 261879\r\n",
      "2 147113\r\n",
      "2 caxh91p\r\n",
      "2 231825\r\n"
     ]
    }
   ],
   "source": [
    "!head {test_filename}.pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1806, [2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds_filename = str(test_filename) + \".pred\"\n",
    "with open(test_preds_filename, 'r') as infile:\n",
    "    lines = infile.readlines()\n",
    "    preds_raw = [line.split()[0] for line in lines]\n",
    "    preds = [int(raw_pred) for raw_pred in preds_raw]\n",
    "len(preds), preds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 1655), (3, 121), (1, 30)]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(preds).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 1666), (3, 80), (1, 60)]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [map_support_to_label(s - 3) for s in test_df.support]\n",
    "Counter(y_true).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "Unsupportive       0.30      0.15      0.20        60\n",
      "     Neutral       0.93      0.93      0.93      1666\n",
      "  Supportive       0.15      0.23      0.18        80\n",
      "\n",
      "   micro avg       0.87      0.87      0.87      1806\n",
      "   macro avg       0.46      0.43      0.44      1806\n",
      "weighted avg       0.88      0.87      0.87      1806\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report(y_true, preds, target_names=['Unsupportive', 'Neutral', 'Supportive']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary classification of supportive replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall support distribution\n",
      "dict_items([(0, 8642), (1, 388)])\n",
      "Train/test split:\n",
      "7224 1806\n",
      "Train support distribution after resampling:\n",
      "[(0, 6905), (1, 1381)]\n",
      "Wrote out train and test files in VW format.\n",
      "Generating 2-grams for all namespaces.\n",
      "final_regressor = /home/levon003/repos/sna-social-support/support_classification/data/vw_support_binary.model\n",
      "Num weight bits = 30\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "creating cache_file = /home/levon003/repos/sna-social-support/support_classification/data/vw_support_binary.train.cache\n",
      "Reading datafile = /home/levon003/repos/sna-social-support/support_classification/data/vw_support_binary.train\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0  -1.0000  -1.0000       22\n",
      "0.000000 0.000000            2            2.0  -1.0000  -1.0000       22\n",
      "0.000000 0.000000            4            4.0  -1.0000  -1.0000       18\n",
      "0.000000 0.000000            8            8.0  -1.0000  -1.0000       28\n",
      "0.000000 0.000000           16           16.0  -1.0000  -1.0000       44\n",
      "0.031250 0.062500           32           32.0  -1.0000  -1.0000      192\n",
      "0.031250 0.031250           64           64.0  -1.0000  -1.0000       50\n",
      "0.031250 0.031250          128          128.0  -1.0000  -1.0000       40\n",
      "0.039062 0.046875          256          256.0  -1.0000  -1.0000      118\n",
      "0.068359 0.097656          512          512.0  -1.0000  -1.0000       20\n",
      "0.047852 0.027344         1024         1024.0  -1.0000  -1.0000       98\n",
      "0.047852 0.047852         2048         2048.0  -1.0000  -1.0000      190\n",
      "0.050781 0.053711         4096         4096.0  -1.0000  -1.0000      162\n",
      "0.048405 0.048405         8192         8192.0  -1.0000  -1.0000       26 h\n",
      "0.041781 0.035165        16384        16384.0   1.0000   1.0000       20 h\n",
      "0.033535 0.025289        32768        32768.0  -1.0000  -1.0000        6 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 7458\n",
      "passes used = 7\n",
      "weighted example sum = 52206.000000\n",
      "weighted label sum = -34748.000000\n",
      "average loss = 0.024155 h\n",
      "best constant = -0.665594\n",
      "best constant's loss = 0.556985\n",
      "total feature number = 4118884\n",
      "\n",
      "Generating 2-grams for all namespaces.\n",
      "only testing\n",
      "predictions = /home/levon003/repos/sna-social-support/support_classification/data/vw_support_binary.test.pred\n",
      "Num weight bits = 30\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = /home/levon003/repos/sna-social-support/support_classification/data/vw_support_binary.test\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0  -1.0000  -1.0000      128\n",
      "0.000000 0.000000            2            2.0  -1.0000  -1.0000       16\n",
      "0.000000 0.000000            4            4.0  -1.0000  -1.0000      202\n",
      "0.000000 0.000000            8            8.0  -1.0000  -1.0000      354\n",
      "0.062500 0.125000           16           16.0  -1.0000  -1.0000      334\n",
      "0.062500 0.062500           32           32.0   1.0000  -1.0000       54\n",
      "0.046875 0.031250           64           64.0  -1.0000  -1.0000      150\n",
      "0.070312 0.093750          128          128.0  -1.0000  -1.0000      154\n",
      "0.066406 0.062500          256          256.0  -1.0000  -1.0000       16\n",
      "0.078125 0.089844          512          512.0  -1.0000  -1.0000       20\n",
      "0.069336 0.060547         1024         1024.0  -1.0000  -1.0000      170\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1806\n",
      "passes used = 1\n",
      "weighted example sum = 1806.000000\n",
      "weighted label sum = -1668.000000\n",
      "average loss = 0.073643\n",
      "best constant = -0.923588\n",
      "best constant's loss = 0.146985\n",
      "total feature number = 147416\n",
      "\n",
      "Loaded predictions: 1806 [0, 0, 0, 0, 0]\n",
      "Prediction support distribution: [(0, 1702), (1, 104)]\n",
      "Test support distribution: [(0, 1737), (1, 69)]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Unsupportive       0.97      0.95      0.96      1737\n",
      "  Supportive       0.19      0.29      0.23        69\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      1806\n",
      "   macro avg       0.58      0.62      0.60      1806\n",
      "weighted avg       0.94      0.93      0.93      1806\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# shuffle the data\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df['support_binary'] = [map_support_to_binary_label(s - 3) for s in df.support]\n",
    "print(\"Overall support distribution\")\n",
    "print(Counter(df.support_binary).items())\n",
    "\n",
    "# identify test data\n",
    "test_percent = 0.2\n",
    "train_df, test_df = sklearn.model_selection.train_test_split(df, test_size=test_percent)\n",
    "print(\"Train/test split:\")\n",
    "print(len(train_df), len(test_df))\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "negative_class_count = Counter(train_df.support_binary)[0]\n",
    "sampling_strategy = {0: int(negative_class_count), \n",
    "                     1: int(negative_class_count * 0.2)}\n",
    "X_resampled, y_resampled = RandomOverSampler(sampling_strategy=sampling_strategy).fit_resample(\n",
    "    train_df.index.values.reshape(-1, 1), \n",
    "    train_df.support_binary)\n",
    "train_df_resampled = train_df.loc[[val[0] for val in X_resampled]]\n",
    "print(\"Train support distribution after resampling:\")\n",
    "print(Counter(train_df_resampled.support_binary).most_common())\n",
    "\n",
    "train_filename = data_dir / 'vw_support_binary.train'\n",
    "test_filename = data_dir / 'vw_support_binary.test'\n",
    "to_vw_format(train_df_resampled, train_filename, binarize=True, discretize=False, add_embeddings=False)\n",
    "to_vw_format(test_df, test_filename, binarize=True, discretize=False, add_embeddings=False)\n",
    "print(\"Wrote out train and test files in VW format.\")\n",
    "\n",
    "!vw -k -c -b 30 --passes 20 --binary {train_filename} -f {data_dir}/vw_support_binary.model --ngram 2\n",
    "print()\n",
    "\n",
    "!vw --binary -t -i {data_dir}/vw_support_binary.model -d {test_filename} -p {test_filename}.pred\n",
    "print()\n",
    "\n",
    "test_preds_filename = str(test_filename) + \".pred\"\n",
    "with open(test_preds_filename, 'r') as infile:\n",
    "    lines = infile.readlines()\n",
    "    preds_raw = [line.split()[0] for line in lines]\n",
    "    preds = [max(int(raw_pred), 0) for raw_pred in preds_raw]\n",
    "print(\"Loaded predictions:\", len(preds), preds[:5])\n",
    "\n",
    "print(\"Prediction support distribution:\", Counter(preds).most_common())\n",
    "\n",
    "y_true = [map_support_to_binary_label(s - 3) for s in test_df.support]\n",
    "print(\"Test support distribution:\", Counter(y_true).most_common())\n",
    "\n",
    "print(sklearn.metrics.classification_report(y_true, preds, target_names=['Unsupportive', 'Supportive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
